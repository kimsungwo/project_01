{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Utils\n",
    "from Utils import read_txt\n",
    "import tools\n",
    "import copy\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import warnings\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import easydict\n",
    "import time\n",
    "\n",
    "# Fix Seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Fix CPU Limitation\n",
    "torch.set_num_threads(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_modal_dataset(Dataset):\n",
    "    def __init__(self, classification_dir, segmentation_dir, txt_COVID, txt_NonCOVID,\n",
    "                 audio_dir, select_num,\n",
    "                 lateral_map=1, min_seg=0.01, transform=None):\n",
    "        \n",
    "        # Dataset Root Directory\n",
    "        self.classification_dir = classification_dir\n",
    "        self.segmentation_dir = segmentation_dir\n",
    "        self.audio_dir = audio_dir\n",
    "        \n",
    "        # Subject\n",
    "        self.txt_path = [txt_COVID, txt_NonCOVID]\n",
    "        self.classes = ['CT_COVID', 'CT_NonCOVID']\n",
    "        self.audio_classes = ['pos', 'neg']\n",
    "        self.num_cls = len(self.classes)\n",
    "\n",
    "        self.img_list = []\n",
    "        self.segment_list = []\n",
    "        self.audio_list = []\n",
    "        \n",
    "        self.min_seg = min_seg\n",
    "        self.select_num = select_num\n",
    "        \n",
    "        for c in range(self.num_cls):\n",
    "            # Classification List\n",
    "            cls_list = [[os.path.join(self.classification_dir, self.classes[c], item), c] for item in\n",
    "                        read_txt(self.txt_path[c])]\n",
    "            self.img_list += cls_list\n",
    "            \n",
    "            # Audio List\n",
    "            a_list = [[os.path.join(audio_dir, self.audio_classes[c], item), c] for item in os.listdir(os.path.join(audio_dir, self.audio_classes[c]))]\n",
    "            self.audio_list += a_list\n",
    "            \n",
    "        # Split Image COVID & Non-COVID\n",
    "        self.img_list = np.array(self.img_list)\n",
    "        pos_index = (self.img_list[:,1]=='0')\n",
    "        neg_index = (self.img_list[:,1]=='1')\n",
    "        pos_img_list = self.img_list[:, 0][pos_index]\n",
    "        neg_img_list = self.img_list[:, 0][neg_index]\n",
    "\n",
    "        # Select Image List\n",
    "        select_pos_img_list = np.random.choice(pos_img_list, self.select_num)\n",
    "        select_neg_img_list = np.random.choice(neg_img_list, self.select_num)\n",
    "\n",
    "        # Mapping Segment List\n",
    "        select_pos_seg_list = []\n",
    "        select_neg_seg_list = []\n",
    "        for s_p_i_l in select_pos_img_list:\n",
    "            select_pos_seg_list.append(os.path.join(segmentation_dir, s_p_i_l.split('/')[-2], \"lateral_map\" + str(lateral_map),\n",
    "                              s_p_i_l.split('/')[-1].replace('.jpg', '.png')))\n",
    "        for s_n_i_l in select_neg_img_list:\n",
    "            select_neg_seg_list.append(os.path.join(segmentation_dir, s_n_i_l.split('/')[-2], \"lateral_map\" + str(lateral_map),\n",
    "                              s_n_i_l.split('/')[-1].replace('.jpg', '.png')))\n",
    "\n",
    "        # Split Audio COIVD & Non-COVID\n",
    "        self.audio_list = np.array(self.audio_list)\n",
    "        pos_index = (self.audio_list[:,1]=='0')\n",
    "        neg_index = (self.audio_list[:,1]=='1')\n",
    "        pos_audio_list = self.audio_list[:, 0][pos_index]\n",
    "        neg_audio_list = self.audio_list[:, 0][neg_index]\n",
    "\n",
    "        # Select Audio List\n",
    "        select_pos_audio_list = np.random.choice(pos_audio_list, select_num)\n",
    "        select_neg_audio_list = np.random.choice(neg_audio_list, select_num)\n",
    "        \n",
    "        # Make DICT\n",
    "        self.data_list = []\n",
    "        for i in range(self.num_cls):\n",
    "            for j in range(self.select_num):\n",
    "                if i == 0:\n",
    "                    data_dict = {'img': select_pos_img_list[j],\n",
    "                                 'seg': select_pos_seg_list[j],\n",
    "                                 'audio': select_pos_audio_list[j],\n",
    "                                 'label': i}\n",
    "                else:\n",
    "                    data_dict = {'img': select_neg_img_list[j],\n",
    "                                 'seg': select_neg_seg_list[j],\n",
    "                                 'audio': select_neg_audio_list[j],\n",
    "                                 'label': i}\n",
    "                self.data_list.append(data_dict)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.select_num*2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Original Data\n",
    "        img_path = self.data_list[idx]['img']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = image.resize((256, 256))\n",
    "\n",
    "        # Segmentation Data\n",
    "        seg_path = self.data_list[idx]['seg']\n",
    "        seg = Image.open(seg_path).convert('RGB')\n",
    "        seg = seg.rotate(-90, expand=True)\n",
    "        seg = seg.resize((256, 256))\n",
    "        \n",
    "        # Audio Data\n",
    "        audio_path = self.data_list[idx]['audio']\n",
    "        audio = Image.open(audio_path).convert('RGB')\n",
    "\n",
    "        # Mask with Original Data\n",
    "        # Step 1 => Segmentation Min-Max Normalization + Min Value(Hyperparameter)\n",
    "        seg_np = np.array(seg)\n",
    "        seg_mask = (seg_np - seg_np.min()) / (seg_np.max() - seg_np.min()) + self.min_seg\n",
    "        # Clip max = 1\n",
    "        seg_mask = np.clip(seg_mask, 0, 1)\n",
    "\n",
    "        # Step 2 => Original Data with seg_mask\n",
    "        image_with_mask = np.multiply(image, seg_mask)\n",
    "\n",
    "        # Step 3 => Change Numpy Dtype => For Using Image Preprocessing\n",
    "        image_with_mask = Image.fromarray(np.uint8(image_with_mask))\n",
    "        \n",
    "        if self.transform:\n",
    "            image_with_mask = self.transform(image_with_mask)\n",
    "            audio = self.transform(audio)\n",
    "        \n",
    "        sample = {'img': image_with_mask,\n",
    "                  'audio': audio,\n",
    "                  'label': int(self.data_list[idx]['label'])}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "normalize = transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                     [0.229, 0.224, 0.225])\n",
    "trans = transforms.Compose([\n",
    "    transforms.Resize((224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "# Train Dataset\n",
    "trainset = multi_modal_dataset(classification_dir='./dataset/image/classfication/',\n",
    "                               segmentation_dir='./dataset/image/classfication/Segmentation/',\n",
    "                               txt_COVID='./dataset/image/classfication/Data-split/COVID/trainCT_COVID.txt',\n",
    "                               txt_NonCOVID='./dataset/image/classfication/Data-split/NonCOVID/trainCT_NonCOVID.txt',\n",
    "                               audio_dir = './dataset/audio/preprocess/train/',\n",
    "                               select_num=500,\n",
    "                               lateral_map=3, min_seg=0.8,\n",
    "                               transform=trans)\n",
    "\n",
    "# Validation Dataset\n",
    "valset = multi_modal_dataset(classification_dir='./dataset/image/classfication/',\n",
    "                               segmentation_dir='./dataset/image/classfication/Segmentation/',\n",
    "                               txt_COVID='./dataset/image/classfication/Data-split/COVID/valCT_COVID.txt',\n",
    "                               txt_NonCOVID='./dataset/image/classfication/Data-split/NonCOVID/valCT_NonCOVID.txt',\n",
    "                               audio_dir = './dataset/audio/preprocess/validation/',\n",
    "                               select_num=100,\n",
    "                               lateral_map=3, min_seg=0.8,\n",
    "                               transform=trans)\n",
    "\n",
    "# Test Dataset\n",
    "testset = multi_modal_dataset(classification_dir='./dataset/image/classfication/',\n",
    "                               segmentation_dir='./dataset/image/classfication/Segmentation/',\n",
    "                               txt_COVID='./dataset/image/classfication/Data-split/COVID/testCT_COVID.txt',\n",
    "                               txt_NonCOVID='./dataset/image/classfication/Data-split/NonCOVID/testCT_NonCOVID.txt',\n",
    "                               audio_dir = './dataset/audio/preprocess/test/',\n",
    "                               select_num=100,\n",
    "                               lateral_map=3, min_seg=0.8,\n",
    "                               transform=trans)\n",
    "\n",
    "# Data Loader\n",
    "multi_train_loader = DataLoader(trainset, batch_size=5, drop_last=True, shuffle=True)\n",
    "multi_val_loader = DataLoader(valset, batch_size=5, drop_last=True, shuffle=False)\n",
    "multi_test_loader = DataLoader(testset, batch_size=5, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_index, batch_samples in enumerate(multi_train_loader):\n",
    "    image, audio, label = batch_samples['img'], batch_samples['audio'], batch_samples['label']\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreTrain Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Load....\n",
      "Image Model Load....\n",
      "Audio Model Load....\n",
      "All Load....\n"
     ]
    }
   ],
   "source": [
    "# DenseNet => Audio, CT BaseLine Model\n",
    "print('Base Model Load....')\n",
    "base_model = models.densenet169(pretrained=True)\n",
    "\n",
    "# Image Model\n",
    "print('Image Model Load....')\n",
    "image_model = models.densenet169(pretrained=True)\n",
    "image_model.classifier = nn.Sequential(nn.Linear(1664, 2), nn.Softmax(dim=1))\n",
    "image_model.load_state_dict(torch.load('./model/single_modality/3_0.8.pt', map_location='cpu'))\n",
    "image_model\n",
    "image_model.eval()\n",
    "\n",
    "# Audio Model\n",
    "print('Audio Model Load....')\n",
    "audio_model = models.densenet169(pretrained=True)\n",
    "audio_model.classifier = nn.Linear(1664, 1)\n",
    "audio_model = nn.Sequential(audio_model, nn.Sigmoid())\n",
    "audio_model.load_state_dict(torch.load('./model/single_modality/audio.pt', map_location='cpu'))\n",
    "audio_model\n",
    "audio_model.eval()\n",
    "\n",
    "print('All Load....')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Feature Extractor List\n",
    "- Output Layer\n",
    "- Transition3\n",
    "- Transition2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgae_feature_extractor = []\n",
    "\n",
    "for child in image_model.children():\n",
    "    imgae_feature_extractor.append(nn.Sequential(child, nn.AvgPool2d((7,7))))\n",
    "    imgae_feature_extractor.append(nn.Sequential(*list(child.children())[:-2], nn.AvgPool2d((7,7))))\n",
    "    imgae_feature_extractor.append(nn.Sequential(*list(child.children())[:-4], nn.AvgPool2d((14,14))))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Feature Extractor List\n",
    "- Output Layer\n",
    "- Transition3\n",
    "- Transition2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_feature_extractor = []\n",
    "\n",
    "for i, (name, module) in enumerate(audio_model[0]._modules.items()):\n",
    "    audio_feature_extractor.append(nn.Sequential(child, nn.AvgPool2d((7,7))))\n",
    "    audio_feature_extractor.append(nn.Sequential(*list(child.children())[:-2], nn.AvgPool2d((7,7))))\n",
    "    audio_feature_extractor.append(nn.Sequential(*list(child.children())[:-4], nn.AvgPool2d((14,14))))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extractor List Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = audio_feature_extractor[0](audio)\n",
    "out2 = audio_feature_extractor[1](audio)\n",
    "out3 = audio_feature_extractor[2](audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1664, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 640, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 256, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete PreTrain Model on Memory\n",
    "del base_model\n",
    "del image_model\n",
    "del audio_model\n",
    "\n",
    "# Delete DataLoader\n",
    "del multi_train_loader\n",
    "del multi_val_loader\n",
    "del multi_test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config => Define Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio Feature\n",
    "# Image Feature\n",
    "# Activation Function\n",
    "\n",
    "def get_possible_layer_configurations():\n",
    "    def get_max_labels():\n",
    "        return (3, 3, 2)\n",
    "\n",
    "    list_conf = []\n",
    "    max_labels = get_max_labels()\n",
    "    \n",
    "    # Audio Feature Extractor => 3\n",
    "    for audio in range(max_labels[0]):\n",
    "        # Image Feature Extractor => 3\n",
    "        for image in range(max_labels[1]):\n",
    "            # Num of Activate List => 3\n",
    "            for activation in range(max_labels[2]):\n",
    "                conf = [audio, image, activation]\n",
    "                list_conf.append(conf)\n",
    "\n",
    "    return list_conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Surrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRecurrentSurrogate(nn.Module):\n",
    "    # number_input_feats => Hidden layer of the Image, Audio, Activation\n",
    "    def __init__(self, num_hidden=100, number_input_feats=3, size_ebedding=100):\n",
    "        super(SimpleRecurrentSurrogate, self).__init__()\n",
    "\n",
    "        self.num_hidden = num_hidden\n",
    "\n",
    "        # input embedding\n",
    "        self.embedding = nn.Sequential(nn.Linear(number_input_feats, size_ebedding),\n",
    "                                       nn.Sigmoid())\n",
    "        # the LSTM\n",
    "        self.lstm = nn.LSTM(size_ebedding, num_hidden)\n",
    "        # The linear layer that maps from hidden state space to output space\n",
    "        self.hid2val = nn.Linear(num_hidden, 1)\n",
    "\n",
    "        self.nonlinearity = nn.Sigmoid()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data.uniform_(-0.1, 0.1)\n",
    "                m.bias.data.fill_(1.8)\n",
    "\n",
    "    def forward(self, sequence_of_operations):\n",
    "        # (seq_len, batch, input_size):\n",
    "\n",
    "        embeds = []\n",
    "        for s in sequence_of_operations:\n",
    "            embeds.append(self.embedding(s))\n",
    "        embeds = torch.stack(embeds, dim=0)\n",
    "\n",
    "        lstm_out, hidden = self.lstm(embeds)\n",
    "\n",
    "        val_space = self.hid2val(lstm_out[-1])\n",
    "        val_space = self.nonlinearity(val_space)\n",
    "\n",
    "        return val_space\n",
    "\n",
    "    def eval_model(self, sequence_of_operations_np, device):\n",
    "        # the user will give this data sample as numpy array (int) with size len_seq x input_size\n",
    "\n",
    "        npseq = np.expand_dims(sequence_of_operations_np, 1)\n",
    "        sequence_of_operations = torch.from_numpy(npseq).float().to(device)\n",
    "        res = self.forward(sequence_of_operations)\n",
    "        res = res.cpu().data.numpy()\n",
    "\n",
    "        return res[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurrogateDataloader():\n",
    "\n",
    "    def __init__(self):\n",
    "        self._dict_data = {}\n",
    "\n",
    "    def add_datum(self, datum_conf, datum_acc):\n",
    "        # data_conf is of size [seq_len, len_data]\n",
    "\n",
    "        seq_len = len(datum_conf)\n",
    "        datum_hash = datum_conf.data.tobytes()\n",
    "\n",
    "        if seq_len in self._dict_data:\n",
    "\n",
    "            if datum_hash in self._dict_data[seq_len]:\n",
    "                # if the configuration is already stored, keep the max accuracy\n",
    "                self._dict_data[seq_len][datum_hash] = (\n",
    "                datum_conf, max(datum_acc, self._dict_data[seq_len][datum_hash][1]))\n",
    "            else:\n",
    "                self._dict_data[seq_len][datum_hash] = (datum_conf, datum_acc)\n",
    "        else:\n",
    "            self._dict_data[seq_len] = {datum_hash: (datum_conf, datum_acc)}\n",
    "\n",
    "    def get_data(self, to_torch=False):\n",
    "        # delivers list of numpy tensors of size [seq_len, num_layers, len_data]\n",
    "\n",
    "        dataset_conf = list()\n",
    "        dataset_acc = list()\n",
    "\n",
    "        for len_key, data_dict in self._dict_data.items():\n",
    "\n",
    "            conf_list = list()\n",
    "            acc_list = list()\n",
    "\n",
    "            for datum_hash, datum in data_dict.items():\n",
    "                conf_list.append(datum[0])\n",
    "                acc_list.append(datum[1])\n",
    "\n",
    "            conf_list = np.transpose(np.asarray(conf_list, np.float32), (1, 0, 2))\n",
    "\n",
    "            dataset_conf.append(np.array(conf_list, np.float32))\n",
    "            dataset_acc.append(np.expand_dims(np.array(acc_list, np.float32), 1))\n",
    "\n",
    "        if to_torch:\n",
    "            for index in range(len(dataset_conf)):\n",
    "                dataset_conf[index] = torch.from_numpy(dataset_conf[index])\n",
    "                dataset_acc[index] = torch.from_numpy(dataset_acc[index])\n",
    "\n",
    "        return dataset_conf, dataset_acc\n",
    "\n",
    "    def get_k_best(self, k):\n",
    "\n",
    "        dataset_conf = list()\n",
    "        dataset_acc = list()\n",
    "\n",
    "        for len_key, data_dict in self._dict_data.items():\n",
    "            for datum_hash, datum in data_dict.items():\n",
    "                dataset_conf.append(datum[0])\n",
    "                dataset_acc.append(datum[1])\n",
    "\n",
    "        dataset_acc = np.array(dataset_acc)\n",
    "        top_k_idx = np.argpartition(dataset_acc, -k)[-k:]\n",
    "\n",
    "        confs = [dataset_conf[i] for i in top_k_idx]\n",
    "        accs = [dataset_acc[i] for i in top_k_idx]\n",
    "\n",
    "        return (confs, accs, top_k_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searchable ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input => Feature Extractor => Fusion Layer => Classification\n",
    "class Searchable_ANN(nn.Module):\n",
    "    def __init__(self, conf, audio_feature_extractor, image_feature_extractor, device):\n",
    "        super(Searchable_ANN, self).__init__()\n",
    "\n",
    "        # conf[0] => image hidden layer\n",
    "        # conf[1] => audio hidden layer\n",
    "        # conf[2] => activation function\n",
    "\n",
    "        self.conf = conf\n",
    "        self.device = device\n",
    "\n",
    "        # Pre-Train Feature Extractor\n",
    "        self.audio_feature_extractor = audio_feature_extractor\n",
    "        self.image_feature_extractor = image_feature_extractor\n",
    "        \n",
    "        # Define Input Size\n",
    "        input_size = [1664, 640, 256]\n",
    "        \n",
    "        # Defin Output Size\n",
    "        self.out_size = 100\n",
    "        \n",
    "        self.alphas = [(input_size[conf[0]], input_size[conf[1]]) for conf in self.conf]\n",
    "\n",
    "        # Define Fuse Layer\n",
    "        self.fusion_layers = self._create_fc_layers()\n",
    "\n",
    "        # Classification => COVID or Non-COVID\n",
    "        self.central_classifier = nn.Sequential(nn.Linear(self.out_size, 1), nn.Sigmoid())\n",
    "\n",
    "    # tensor_tuple => CSF, PET, SMRI\n",
    "    def forward(self, image, audio):\n",
    "        # Image Feature\n",
    "        image_features = [self.image_feature_extractor[0].to(self.device)(image.to(self.device)).squeeze(), \n",
    "                          self.image_feature_extractor[1].to(self.device)(image.to(self.device)).squeeze(),\n",
    "                          self.image_feature_extractor[2].to(self.device)(image.to(self.device)).squeeze()]\n",
    "        \n",
    "        image_features = [image_features[idx] for idx in self.conf[:, 0]]\n",
    "\n",
    "        # Audio Feature\n",
    "        audio_features = [self.audio_feature_extractor[0].to(self.device)(audio.to(self.device)).squeeze(),\n",
    "                          self.audio_feature_extractor[1].to(self.device)(audio.to(self.device)).squeeze(),\n",
    "                          self.audio_feature_extractor[2].to(self.device)(audio.to(self.device)).squeeze()]\n",
    "        \n",
    "        audio_features = [audio_features[idx] for idx in self.conf[:, 1]]\n",
    "\n",
    "        # Fusion Feature\n",
    "        for fusion_idx, conf in enumerate(self.conf):\n",
    "            image_feat = image_features[fusion_idx]\n",
    "            audio_feat = audio_features[fusion_idx]\n",
    "\n",
    "            if fusion_idx == 0:\n",
    "                fused = torch.cat((image_feat, audio_feat), 1)\n",
    "                out = self.fusion_layers[fusion_idx](fused)\n",
    "\n",
    "            else:\n",
    "                fused = torch.cat((image_feat, audio_feat, out), 1)\n",
    "                out = self.fusion_layers[fusion_idx](fused)\n",
    "\n",
    "        # Dropout with Classification\n",
    "        out = self.central_classifier(out)\n",
    "        return out\n",
    "\n",
    "    def central_params(self):\n",
    "        central_parameters = [\n",
    "            {'params': self.fusion_layers.parameters()},\n",
    "            {'params': self.central_classifier.parameters()}\n",
    "        ]\n",
    "\n",
    "        return central_parameters\n",
    "\n",
    "    def _create_fc_layers(self):\n",
    "        fusion_layers = []\n",
    "\n",
    "        for i, conf in enumerate(self.conf):\n",
    "            in_size = sum(self.alphas[i])\n",
    "\n",
    "            # args.inner_representation_size => ANN Output Size\n",
    "            if i > 0:\n",
    "                in_size += self.out_size\n",
    "\n",
    "            out_size = self.out_size\n",
    "\n",
    "            # Activation Function\n",
    "            if conf[2] == 0:\n",
    "                nl = nn.Sigmoid()\n",
    "            elif conf[2] == 1:\n",
    "                nl = nn.ReLU()\n",
    "\n",
    "            op = nn.Sequential(nn.Linear(in_size, out_size), nl)\n",
    "            fusion_layers.append(op)\n",
    "\n",
    "        return nn.ModuleList(fusion_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scheduler**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRCosineAnnealingScheduler():\n",
    "\n",
    "    def __init__(self, eta_max, eta_min, Ti, Tmultiplier, num_batches_per_epoch):\n",
    "\n",
    "        self.eta_min = eta_min\n",
    "        self.eta_max = eta_max\n",
    "        self.Ti = Ti\n",
    "        self.Tcur = 0.0\n",
    "        self.nbpe = num_batches_per_epoch\n",
    "        self.iteration_counter = 0.0\n",
    "        self.eta = eta_max\n",
    "        self.Tm = Tmultiplier\n",
    "\n",
    "    def _compute_rule(self):\n",
    "        self.eta = self.eta_min + 0.5 * (self.eta_max - self.eta_min) * (1 + np.cos(np.pi * self.Tcur / self.Ti))\n",
    "        return self.eta\n",
    "\n",
    "    def step(self):\n",
    "\n",
    "        self.Tcur = self.iteration_counter / self.nbpe\n",
    "        self.iteration_counter = self.iteration_counter + 1.0\n",
    "        eta = self._compute_rule()\n",
    "\n",
    "        if eta <= self.eta_min + 1e-10:\n",
    "            self.Tcur = 0\n",
    "            self.Ti = self.Ti * self.Tm\n",
    "            self.iteration_counter = 0\n",
    "\n",
    "        return eta\n",
    "\n",
    "    def update_optimizer(self, optimizer):\n",
    "        state_dict = optimizer.state_dict()\n",
    "        for param_group in state_dict['param_groups']:\n",
    "            param_group['lr'] = self.eta\n",
    "        optimizer.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ntu_track_acc(model, criteria, optimizer, scheduler, dataloaders, dataset_sizes, device=None, num_epochs=5):\n",
    "    \n",
    "    best_model_sd = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'dev']: \n",
    "\n",
    "            if phase == 'train':\n",
    "                if not isinstance(scheduler, LRCosineAnnealingScheduler):\n",
    "                    scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for data in dataloaders[phase]:     \n",
    "                \n",
    "                # get the inputs\n",
    "                image, audio, label = data['img'], data['audio'], data['label']                \n",
    "                \n",
    "                # device\n",
    "                image = image.to(device)\n",
    "                audio = audio.to(device)                \n",
    "                label = label.to(device)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()     \n",
    "                \n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    output = model(image, audio)\n",
    "                    loss = criteria(output, label.float().reshape(-1,1))\n",
    "                    \n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        scheduler.step()\n",
    "                        scheduler.update_optimizer(optimizer)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                # statistics\n",
    "                running_loss += loss.item() * image.size(0)\n",
    "                output[output<=0.5] = 0\n",
    "                output[output>0.5] = 1\n",
    "                \n",
    "                running_corrects += torch.sum(output.squeeze() == label.data)\n",
    "        \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc  = running_corrects.double() / dataset_sizes[phase]\n",
    "    \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'dev' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                \n",
    "    model.train(False)            \n",
    "    \n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeltype => conf, audio_feature_extractor, image_feature_extractor, device\n",
    "def train_sampled_models(sampled_configurations, searchable_type, dataloaders,\n",
    "                         args, device,\n",
    "                         audio_feature_extractor, image_feature_extractor):\n",
    "    \n",
    "    dataset_sizes = {x: len(dataloaders[x].dataset) for x in ['train', 'dev']}\n",
    "    num_batches_per_epoch = dataset_sizes['train'] / args.batchsize\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    real_accuracies = []\n",
    "\n",
    "    for idx, configuration in enumerate(sampled_configurations):\n",
    "\n",
    "        rmode = searchable_type(configuration, audio_feature_extractor, image_feature_extractor, device)\n",
    "        params = rmode.parameters()\n",
    "\n",
    "        # optimizer and scheduler\n",
    "        optimizer = optim.Adam(params, lr=args.eta_max, weight_decay=1e-4)\n",
    "        scheduler = LRCosineAnnealingScheduler(args.eta_max, args.eta_min, args.Ti, args.Tm,\n",
    "                                                      num_batches_per_epoch)\n",
    "        rmode.to(device)\n",
    "        print('Now training: ')\n",
    "        print(configuration)\n",
    "\n",
    "        best_model_acc = train_ntu_track_acc(rmode, criterion, optimizer, scheduler, dataloaders,\n",
    "                                                        dataset_sizes,\n",
    "                                                        device=device, num_epochs=args.epochs)\n",
    "        # Append Result\n",
    "        real_accuracies.append(best_model_acc)\n",
    "\n",
    "    return real_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSearcher():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def search(self):\n",
    "        pass\n",
    "\n",
    "    def _epnas(self, model_type, surrogate_dict, dataloaders, dataset_searchmethods, device, audio_feature_extractor, image_feature_extractor):\n",
    "\n",
    "        # surrogate\n",
    "        surrogate = surrogate_dict['model']\n",
    "        s_crite = surrogate_dict['criterion']\n",
    "        s_data = SurrogateDataloader()\n",
    "        s_optim = optim.Adam(surrogate.parameters(), lr=0.001)\n",
    "\n",
    "        # search functions that are specific to the dataset\n",
    "        train_sampled_models = dataset_searchmethods['train_sampled']\n",
    "        get_possible_layer_configurations = dataset_searchmethods['get_layer_confs']\n",
    "\n",
    "        temperature = 10.0\n",
    "\n",
    "        sampled_k_confs = []\n",
    "\n",
    "        shared_weights = dict()\n",
    "\n",
    "        # repeat process search_iterations times\n",
    "        for si in range(2):\n",
    "            print(50 * \"=\")\n",
    "            print(\"Search iteration {}/{} \".format(si, 3))\n",
    "\n",
    "            # for each fusion\n",
    "            for progression_index in range(2):\n",
    "\n",
    "                print(25 * \"-\")\n",
    "                print(\"Progressive step {}/{} \".format(progression_index, 3))\n",
    "\n",
    "                # Step 1: unfold layer (fusion index)\n",
    "                list_possible_layer_confs = get_possible_layer_configurations()\n",
    "\n",
    "                # Step 2: merge previous top with unfolded configurations\n",
    "                all_configurations = tools.merge_unfolded_with_sampled(sampled_k_confs, list_possible_layer_confs,\n",
    "                                                                       progression_index)\n",
    "\n",
    "                # Step 3: obtain accuracies for all possible unfolded configurations\n",
    "                # if first execution, just train all, if not, use surrogate to predict them\n",
    "                if si + progression_index == 0:\n",
    "                    all_accuracies = train_sampled_models(all_configurations, model_type, dataloaders, self.args, device, audio_feature_extractor, image_feature_extractor)\n",
    "                    tools.update_surrogate_dataloader(s_data, all_configurations, all_accuracies)\n",
    "                    tools.train_surrogate(surrogate, s_data, s_optim, s_crite, self.args, device)\n",
    "\n",
    "                    print(\"Trained architectures: \")\n",
    "                    print(list(zip(all_configurations, all_accuracies)))\n",
    "                    \n",
    "                else:\n",
    "                    all_accuracies = tools.predict_accuracies_with_surrogate(all_configurations, surrogate, device)\n",
    "                    print(\"Predicted accuracies: \")\n",
    "                    print(list(zip(all_configurations, all_accuracies)))\n",
    "\n",
    "                # Step 4: sample K architectures and train them. \n",
    "                # this should happen only if not first iteration because in that case, \n",
    "                # all confs were trained in step 3\n",
    "                if si + progression_index == 0:\n",
    "                    sampled_k_confs = tools.sample_k_configurations(all_configurations, all_accuracies,\n",
    "                                                                    self.args.num_samples, temperature)\n",
    "\n",
    "                    estimated_accuracies = tools.predict_accuracies_with_surrogate(all_configurations, surrogate,\n",
    "                                                                                       device)\n",
    "                    diff = np.abs(np.array(estimated_accuracies) - np.array(all_accuracies))\n",
    "                    print(\"Error on accuracies = {}\".format(diff))\n",
    "\n",
    "                else:\n",
    "                    sampled_k_confs = tools.sample_k_configurations(all_configurations, all_accuracies,\n",
    "                                                                    self.args.num_samples, temperature)\n",
    "                    sampled_k_accs = train_sampled_models(sampled_k_confs, model_type, dataloaders, self.args, device, audio_feature_extractor, image_feature_extractor)\n",
    "\n",
    "                    tools.update_surrogate_dataloader(s_data, sampled_k_confs, sampled_k_accs)\n",
    "                    err = tools.train_surrogate(surrogate, s_data, s_optim, s_crite, self.args, device)\n",
    "\n",
    "                    print(\"Trained architectures: \")\n",
    "                    print(list(zip(sampled_k_confs, sampled_k_accs)))\n",
    "                    print(\"with surrogate error: {}\".format(err))\n",
    "\n",
    "                # temperature decays at each step\n",
    "                iteration = si * self.args.search_iterations + progression_index\n",
    "                temperature = tools.compute_temperature(iteration, self.args)\n",
    "                print(\"Temperature is being set to {}\".format(temperature))\n",
    "\n",
    "        return s_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Modal_Searcher(ModelSearcher):\n",
    "    def __init__(self, args, trainset, valset, device, audio_feature_extractor, image_feature_extractor):\n",
    "        super(Multi_Modal_Searcher, self).__init__(args)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        datasets = {'train': trainset, 'dev': valset}\n",
    "        \n",
    "        self.dataloaders = {\n",
    "            x: DataLoader(datasets[x], batch_size=args.batchsize, shuffle=True, num_workers=1,\n",
    "                          drop_last=True) for x in ['train', 'dev']}\n",
    "        self.audio_feature_extractor = audio_feature_extractor\n",
    "        self.image_feature_extractor = image_feature_extractor\n",
    "\n",
    "    def search(self):\n",
    "        surrogate = SimpleRecurrentSurrogate(100, 3, 100)\n",
    "        surrogate.to(self.device)\n",
    "        surrogate_dict = {'model': surrogate, 'criterion': torch.nn.MSELoss()}\n",
    "        ntu_searchmethods = {'train_sampled': train_sampled_models,\n",
    "                             'get_layer_confs': get_possible_layer_configurations}\n",
    "\n",
    "        return self._epnas(Searchable_ANN, surrogate_dict, self.dataloaders, ntu_searchmethods, self.device, self.audio_feature_extractor, self.image_feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search NAS by MFAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({ \"epochs\": 2, \"search_iterations\": 3,\n",
    "                          \"eta_max\": 0.001, \"eta_min\": 0.000001, \"Ti\": 1, \"Tm\": 2,\n",
    "                          \"batchsize\": 10, \"num_samples\": 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MFAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntu_searcher = Multi_Modal_Searcher(args, trainset, valset, device, audio_feature_extractor, imgae_feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFAS for NTU Started!!!!\n",
      "==================================================\n",
      "Search iteration 0/3 \n",
      "-------------------------\n",
      "Progressive step 0/3 \n",
      "Now training: \n",
      "[[0 0 0]]\n",
      "train Loss: 0.3103 Acc: 0.8830\n",
      "dev Loss: 0.3683 Acc: 0.8550\n"
     ]
    }
   ],
   "source": [
    "print(\"MFAS for NTU Started!!!!\")\n",
    "start_time = time.time()\n",
    "surrogate_data = ntu_searcher.search()\n",
    "time_elapsed = time.time() - start_time\n",
    "print('Search complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
